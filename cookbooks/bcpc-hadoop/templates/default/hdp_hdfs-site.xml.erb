<?xml version="1.0"?>
<!--
################################################
#
#              Generated by Chef
#
################################################
-->
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration xmlns:xi="http://www.w3.org/2001/XInclude">
  <property>
    <name>dfs.replication</name>
    <value><%= node[:bcpc][:hadoop][:hdfs][:dfs_replication_factor] %></value>
  </property>

  <property>
    <name>dfs.name.dir</name>
    <value><%=@mounts.map{ |d| "file:///disk/#{d}/dfs/namedir" }.join(",")%></value>
  </property>

  <property>
    <name>dfs.namenode.name.dir</name>
    <value><%=@mounts.map{ |d| "file:///disk/#{d}/dfs/nn" }.join(",")%></value>
  </property>

  <property>
    <name>dfs.nameservices</name>
    <value><%= node.chef_environment %></value>
  </property>

  <property>
    <name>dfs.ha.namenodes.<%= node.chef_environment %></name>
    <value><%=@nn_hosts.map{ |s| "namenode#{s[:node_number]}" }.join(",") %></value>
  </property>

  <property>
    <name>dfs.datanode.data.dir</name>
    <value><%=@mounts.map{ |d| "file:///disk/#{d}/dfs/dn" }.join(",")%></value>
  </property>

  <property>
    <name>dfs.datanode.failed.volumes.tolerated</name>
    <value><%= node[:bcpc][:hadoop][:hdfs][:failed_volumes_tolerated] %></value>
  </property>

  <% @nn_hosts.each do |h| %>	
    <property>
      <name>dfs.namenode.rpc-address.<%= "#{node.chef_environment}.namenode#{h[:node_number]}" %></name>
      <value><%=float_host(h[:hostname]) %>:8020</value>
    </property>
  <% end %>

  <% @nn_hosts.each do |h| %>	
    <property>
      <name>dfs.namenode.http-address.<%= "#{node.chef_environment}.namenode#{h[:node_number]}" %></name>
      <value><%=float_host(h[:hostname]) %>:50070</value>
    </property>
  <% end %>

  <% @nn_hosts.each do |h| %>	
    <property>
      <name>dfs.namenode.https-address.<%= "#{node.chef_environment}.namenode#{h[:node_number]}" %></name>
      <value><%=float_host(h[:hostname]) %>:50080</value>
    </property>
  <% end %>

  <property>
    <name>dfs.client.failover.proxy.provider.<%= node.chef_environment %></name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>

  <% if node[:bcpc][:hadoop][:hdfs][:HA] == true then %>
  <%=  '<xi:include href="file:///etc/hadoop/conf/hdfs-site_HA.xml"/>' %>
  <% end %>

  <property>
    <name>dfs.webhdfs.enabled</name>
    <value>true</value>
  </property>

  <property>
    <name>dfs.client.read.shortcircuit</name>
    <value>true</value>
  </property>

  <property>
    <name>dfs.domain.socket.path</name>
    <value>/var/run/hadoop-hdfs/dn._PORT</value>
  </property>

  <property>
    <name>dfs.client.file-block-storage-locations.timeout</name>
    <value>3000</value>
  </property>

  <property>
    <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
    <value>true</value>
  </property>

  <property>
    <name>dfs.datanode.dns.interface</name>
    <value><%= node['bcpc']['floating']['interface'] %></value>
  </property>

  <property>
    <name>dfs.client.local.interfaces</name>
    <value><%= node['bcpc']['floating']['interface'] %></value>
  </property>

  <property>
    <name>dfs.namenode.avoid.read.stale.datanode</name>
    <value>true</value>
  </property>

  <property>
    <name>dfs.namenode.avoid.write.stale.datanode</name>
    <value>true</value>
  </property>

  <property>
    <name>dfs.hosts.exclude</name>
    <value>/etc/hadoop/conf/dfs.exclude</value>
  </property>

  <property>
    <name>dfs.datanode.du.reserved</name>
    <!-- Reserve 1GB on each datanode -->
    <value>1073741824</value>
  </property>

  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value><%="/disk/#{@mounts[0]}/dfs/jn"%></value>
  </property>

  <property>
    <name>dfs.blocksize</name>
    <value><%= node["bcpc"]["hadoop"]["hdfs"]["dfs_blocksize"] %></value>
  </property>
</configuration>
